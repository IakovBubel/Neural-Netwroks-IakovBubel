{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**"
      ],
      "metadata": {
        "id": "bsISED2LPWBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class GrainImageDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None, mode='train'):\n",
        "\n",
        "        self.dataframe = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "\n",
        "        self.class_to_index = {'wheat': 0, 'oats': 1, 'flax': 2, 'barley': 3}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.dataframe.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            class_name = self.dataframe.iloc[idx, 1]\n",
        "            label = self.class_to_index[class_name]\n",
        "            label = torch.tensor(label, dtype=torch.long)\n",
        "        else:\n",
        "            # Dummy label (-1), поскольку test.csv не содержит классов\n",
        "            label = torch.tensor(-1, dtype=torch.long)\n",
        "        return image, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao00Ztgn7Rcy",
        "outputId": "57b3d989-2d5b-4bf1-c6fc-59d1e0452953"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Architecture**"
      ],
      "metadata": {
        "id": "gZ5tfb2gPbfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "class GrainClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GrainClassifier, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Input channels = 3 (RGB)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(256 * 8 * 8, 512)\n",
        "        self.fc2 = nn.Linear(512, 4)  # 4 classes: wheat, oats, flax, barley\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "\n",
        "        x = x.view(-1, 256 * 8 * 8)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "index_to_class = {0: 'wheat', 1: 'oats', 2: 'flax', 3: 'barley'}\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_csv_path = '/content/drive/MyDrive/NeuralNetworks/train/train.csv'\n",
        "test_csv_path = '/content/drive/MyDrive/NeuralNetworks/test/test.csv'\n",
        "train_img_dir = '/content/drive/MyDrive/NeuralNetworks/train/images'\n",
        "test_img_dir = '/content/drive/MyDrive/NeuralNetworks/test/images'\n",
        "\n",
        "train_dataset = GrainImageDataset(csv_file=train_csv_path, img_dir=train_img_dir, transform=transform, mode='train')\n",
        "test_dataset = GrainImageDataset(csv_file=test_csv_path, img_dir=test_img_dir, transform=transform, mode='test')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "num_classes = 4\n",
        "model = GrainClassifier().to(device)"
      ],
      "metadata": {
        "id": "a5kCZbDB7RWN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimizer**"
      ],
      "metadata": {
        "id": "L_LWLONOPjS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "mFwNhdH-7RGq"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss function**"
      ],
      "metadata": {
        "id": "oW7FuNU7Plm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "hRt7Dr9G7Q-E"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "F20M1MFSPoRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(model, train_loader, loss_func, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = loss_func(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Execute training\n",
        "train(model, train_loader, loss_func, optimizer, num_epochs=10, device=device)"
      ],
      "metadata": {
        "id": "eQ80eBTH7QyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7016d21-5fc5-4114-9d05-b7cfa6cc33fa"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.1995505736424372\n",
            "Epoch 2/10, Loss: 1.0483199672384576\n",
            "Epoch 3/10, Loss: 0.9327336890356881\n",
            "Epoch 4/10, Loss: 0.8583368868618221\n",
            "Epoch 5/10, Loss: 0.7726396983796424\n",
            "Epoch 6/10, Loss: 0.7647321892308665\n",
            "Epoch 7/10, Loss: 0.7444684338438642\n",
            "Epoch 8/10, Loss: 0.6840629541611933\n",
            "Epoch 9/10, Loss: 0.617866656937442\n",
            "Epoch 10/10, Loss: 0.5915774896249666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "Mv97YI5mPqM4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ZDGNoip8nxhd"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.cpu().tolist())\n",
        "    return predictions\n",
        "\n",
        "# Execute evaluation\n",
        "predictions = evaluate(model, test_loader, device=device)\n",
        "\n",
        "# Convert numeric predictions to class names\n",
        "class_predictions = [index_to_class[pred] for pred in predictions]\n",
        "\n",
        "# Load the test dataframe, add class predictions, and save to a new CSV\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "test_df['class'] = class_predictions\n",
        "test_df.to_csv('/content/drive/MyDrive/NeuralNetworks/predictions.csv', index=False)\n",
        "\n"
      ]
    }
  ]
}